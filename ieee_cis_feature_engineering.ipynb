{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b17c65a",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering — IEEE-CIS Fraud Detection\n",
    "\n",
    "This notebook is a **senior data-science reference guide** to feature engineering\n",
    "for tabular fraud detection. It demonstrates every major technique category\n",
    "with concrete implementations and explains *why* each technique works.\n",
    "\n",
    "## Dataset\n",
    "The [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection)\n",
    "dataset consists of:\n",
    "- **Transaction table** — ~590 k rows × 394 columns\n",
    "  (TransactionDT, TransactionAmt, ProductCD, card1–6, addr, C/D/M/V features)\n",
    "- **Identity table** — ~144 k rows × 41 columns\n",
    "  (DeviceType, DeviceInfo, id_01–id_38)\n",
    "\n",
    "Because the dataset requires Kaggle authentication, this notebook uses a\n",
    "**synthetic replica** (`load_synthetic_ieee`) that mirrors the real dataset's\n",
    "schema, missing-value patterns, cardinalities, and class imbalance (~3.5 % fraud).\n",
    "The same code runs unchanged on the real CSV files — just swap the loader.\n",
    "\n",
    "## Techniques covered\n",
    "| Section | Technique |\n",
    "|---------|-----------|\n",
    "| §3 | Missingness analysis & heuristic imputation |\n",
    "| §4 | Frequency encoding & smoothed target (mean) encoding |\n",
    "| §5 | Hand-crafted interaction features |\n",
    "| §6 | Entity-level aggregation (group statistics) |\n",
    "| §7 | Temporal feature extraction + cyclical sin/cos encoding |\n",
    "| §8 | PCA compression of correlated V-feature blocks |\n",
    "| §9 | Feature selection: variance, correlation pruning, permutation importance |\n",
    "| §10 | Baseline vs. engineered-features model lift |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5662fe",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))   # allow notebook to find the package\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ── project helpers ──────────────────────────────────────────────────────────\n",
    "from feature_engineering.preprocessing import (\n",
    "    load_synthetic_ieee,\n",
    "    reduce_mem_usage,\n",
    "    summarise_missing,\n",
    "    impute_by_type,\n",
    ")\n",
    "from feature_engineering.encoders import TargetEncoder, FrequencyEncoder\n",
    "from feature_engineering.aggregations import GroupAggregator\n",
    "from feature_engineering.temporal import extract_temporal_features, extract_email_features\n",
    "from feature_engineering.selection import (\n",
    "    drop_high_correlation,\n",
    "    variance_threshold_select,\n",
    "    importance_select,\n",
    "    pca_compress,\n",
    ")\n",
    "from feature_engineering.utils import (\n",
    "    plot_missing,\n",
    "    plot_feature_importance,\n",
    "    roc_comparison,\n",
    "    plot_pca_explained_variance,\n",
    "    evaluate_model,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "print(\"All imports OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbe09c",
   "metadata": {},
   "source": [
    "## 2. Load Synthetic IEEE-CIS Data\n",
    "\n",
    "`load_synthetic_ieee` returns two DataFrames with the exact same schema as\n",
    "the Kaggle competition files.\n",
    "To use the **real** data, replace the cell below with:\n",
    "\n",
    "```python\n",
    "trans    = pd.read_csv(\"train_transaction.csv\")\n",
    "identity = pd.read_csv(\"train_identity.csv\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans, identity = load_synthetic_ieee(n_rows=150_000, seed=42)\n",
    "\n",
    "print(f\"Transactions : {trans.shape[0]:,} rows × {trans.shape[1]} columns\")\n",
    "print(f\"Identity     : {identity.shape[0]:,} rows × {identity.shape[1]} columns\")\n",
    "print(f\"\\nFraud rate   : {trans['isFraud'].mean() * 100:.2f} %\")\n",
    "trans.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge identity on TransactionID (left join — most approaches merge here)\n",
    "df = trans.merge(identity, on=\"TransactionID\", how=\"left\")\n",
    "print(f\"Merged shape : {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "\n",
    "# Downcast dtypes immediately to reduce RAM\n",
    "df = reduce_mem_usage(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884114dd",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Before engineering features we need to understand:\n",
    "1. Class imbalance\n",
    "2. Missingness patterns\n",
    "3. Distribution of the key numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b87804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Class imbalance ──────────────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "fraud_counts = df[\"isFraud\"].value_counts()\n",
    "axes[0].bar([\"Legit\", \"Fraud\"], fraud_counts.values,\n",
    "            color=[\"steelblue\", \"tomato\"], edgecolor=\"white\")\n",
    "axes[0].set_title(\"Class distribution\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "for i, v in enumerate(fraud_counts.values):\n",
    "    axes[0].text(i, v + 200, f\"{v:,}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# TransactionAmt distribution (log scale) by class\n",
    "legit = df.loc[df[\"isFraud\"] == 0, \"TransactionAmt\"].clip(upper=5000)\n",
    "fraud = df.loc[df[\"isFraud\"] == 1, \"TransactionAmt\"].clip(upper=5000)\n",
    "axes[1].hist(legit, bins=80, alpha=0.6, label=\"Legit\", color=\"steelblue\", density=True)\n",
    "axes[1].hist(fraud, bins=80, alpha=0.6, label=\"Fraud\", color=\"tomato\", density=True)\n",
    "axes[1].set_xlabel(\"TransactionAmt (clipped at 5 000)\")\n",
    "axes[1].set_title(\"Transaction amount distribution by class\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Missingness ──────────────────────────────────────────────────────────────\n",
    "# Focus on columns with at least 1 % missing for readability\n",
    "miss_summary = summarise_missing(df)\n",
    "print(\"Top 20 columns by missingness:\")\n",
    "display(miss_summary[miss_summary[\"pct_missing\"] > 1].head(20))\n",
    "\n",
    "plot_missing(df, figsize=(11, 6), max_cols=35,\n",
    "             title=\"Missing-value rates (top 35 columns)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Correlation heatmap (key numeric features only) ──────────────────────────\n",
    "key_cols = [\"TransactionAmt\", \"C1\", \"C2\", \"C5\", \"C6\",\n",
    "            \"D1\", \"D2\", \"D4\", \"D10\", \"D15\", \"isFraud\"]\n",
    "key_cols = [c for c in key_cols if c in df.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df[key_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "            center=0, ax=ax, linewidths=0.5)\n",
    "ax.set_title(\"Pearson correlation — key numeric features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb1c40a",
   "metadata": {},
   "source": [
    "## 4. Missing-Value Imputation\n",
    "\n",
    "### Why is this critical for fraud detection?\n",
    "\n",
    "The IEEE-CIS dataset contains **block-wise missingness** in the V-features:\n",
    "columns within the same block are either all observed or all missing for a\n",
    "given transaction. This means missingness itself is a strong signal —\n",
    "a transaction with V1–V11 missing likely came from a different device\n",
    "fingerprinting pathway than one with all V columns observed.\n",
    "\n",
    "### Strategy: -999 sentinel for tree-based models\n",
    "\n",
    "For gradient-boosted trees and random forests, the best practice is to fill\n",
    "numeric NaNs with a sentinel value (`-999`) that lies far outside the\n",
    "natural range. The tree can then create a split `feature < -998.5` to\n",
    "distinguish missing from non-missing, effectively learning the missingness\n",
    "signal automatically.\n",
    "\n",
    "For linear models and neural networks, use **median** imputation + add a\n",
    "binary `{col}_was_nan` indicator column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Add was_nan indicators for key partially-missing columns ─────────────────\n",
    "# This preserves the missingness signal for linear models\n",
    "high_miss_cols = (\n",
    "    miss_summary[miss_summary[\"pct_missing\"].between(5, 95)]\n",
    "    .index[:20].tolist()\n",
    ")\n",
    "# Only numeric columns\n",
    "high_miss_cols = [c for c in high_miss_cols\n",
    "                  if c in df.select_dtypes(include=np.number).columns]\n",
    "\n",
    "for col in high_miss_cols:\n",
    "    df[f\"{col}_was_nan\"] = df[col].isna().astype(np.int8)\n",
    "\n",
    "print(f\"Added {len(high_miss_cols)} *_was_nan indicator columns.\")\n",
    "\n",
    "# ── Impute: -999 sentinel for trees ─────────────────────────────────────────\n",
    "df = impute_by_type(df, cat_fill=\"Unknown\", num_strategy=\"minus999\")\n",
    "\n",
    "print(\"\\nRemaining missing values after imputation:\",\n",
    "      df.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76972f11",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding Strategies\n",
    "\n",
    "Raw categorical columns like `card4` (\"visa\", \"mastercard\") and\n",
    "`P_emaildomain` (\"gmail.com\", \"anonymous.com\") contain predictive\n",
    "information that must be encoded numerically.\n",
    "\n",
    "| Encoding | Best for | Risk |\n",
    "|----------|----------|------|\n",
    "| One-hot | Low-cardinality (<20 levels) | Dimensionality explosion for high-card |\n",
    "| **Frequency** | High-cardinality, no target leakage | Loses within-category target info |\n",
    "| **Target (mean)** | High-cardinality, strong signal | Leakage → use LOO or CV folds |\n",
    "| Label | Trees only (preserves ordinality) | Ordinal assumption may be wrong |\n",
    "\n",
    "### Frequency encoding\n",
    "Replace each category with its fractional frequency.\n",
    "`P_emaildomain = \"gmail.com\"` → 0.35 (35 % of transactions used Gmail).\n",
    "\n",
    "### Smoothed target encoding with leave-one-out\n",
    "Replace each category with the smoothed conditional fraud rate.\n",
    "`P_emaildomain = \"anonymous.com\"` → 0.12 (12 % of anonymous.com transactions were fraud).\n",
    "\n",
    "LOO: when encoding the *training set*, each row uses the group mean computed\n",
    "**excluding itself**. This prevents the model from learning a trivially\n",
    "perfect mapping `encoded_val == y`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8fadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to encode\n",
    "cat_cols = [\"ProductCD\", \"card4\", \"card6\",\n",
    "            \"P_emaildomain\", \"R_emaildomain\",\n",
    "            \"DeviceType\"]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "# ── Train / test split (before any fitting) ───────────────────────────────\n",
    "# IMPORTANT: fit encoders ONLY on train to prevent leakage\n",
    "y = df[\"isFraud\"]\n",
    "X = df.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Test  fraud rate: {y_test.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Frequency encoding ───────────────────────────────────────────────────────\n",
    "freq_enc = FrequencyEncoder(cols=cat_cols)\n",
    "X_train = freq_enc.fit_transform(X_train)   # fit on train only\n",
    "X_test  = freq_enc.transform(X_test)\n",
    "\n",
    "print(\"Frequency-encoded columns added:\")\n",
    "for col in cat_cols:\n",
    "    sample = X_train[f\"{col}_freq\"].describe()\n",
    "    print(f\"  {col}_freq: mean={sample['mean']:.4f}, \"\n",
    "          f\"min={sample['min']:.4f}, max={sample['max']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c06bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Target encoding (LOO on train, smoothed mean on test) ───────────────────\n",
    "high_card_cols = [\"P_emaildomain\", \"R_emaildomain\", \"card4\", \"card6\", \"ProductCD\"]\n",
    "high_card_cols = [c for c in high_card_cols if c in X_train.columns]\n",
    "\n",
    "te_enc = TargetEncoder(cols=high_card_cols, smoothing=20, noise_level=0.01)\n",
    "X_train = te_enc.fit_transform(X_train, y_train)   # LOO on train\n",
    "X_test  = te_enc.transform(X_test)                  # smoothed mean on test\n",
    "\n",
    "print(\"Target-encoded columns (fraud rate per category, smoothed):\")\n",
    "for col in high_card_cols:\n",
    "    new_col = f\"{col}_te\"\n",
    "    print(f\"  {new_col}: mean={X_train[new_col].mean():.4f}, \"\n",
    "          f\"std={X_train[new_col].std():.4f}\")\n",
    "\n",
    "# Peek at what the encoder learned for P_emaildomain\n",
    "if \"P_emaildomain\" in te_enc._mapping:\n",
    "    print(\"\\nTop 10 P_emaildomain fraud rates (smoothed):\")\n",
    "    display(te_enc._mapping[\"P_emaildomain\"].sort_values(ascending=False).head(10).to_frame(\"fraud_rate\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd954fa7",
   "metadata": {},
   "source": [
    "## 6. Hand-Crafted Interaction Features\n",
    "\n",
    "Domain knowledge about fraud patterns motivates specific feature combinations.\n",
    "These are features that a model *could* discover, but providing them explicitly\n",
    "reduces the sample complexity required.\n",
    "\n",
    "| Feature | Intuition |\n",
    "|---------|-----------|\n",
    "| `amt_div_card1_mean_amt` | Is this transaction unusually large for this card? |\n",
    "| `addr_mismatch` | Purchaser address ≠ recipient address → risky |\n",
    "| `is_round_amount` | Fraudsters sometimes test with round amounts |\n",
    "| `log_amt` | Log-transform compresses the heavy right tail |\n",
    "| `amt_cents` | Cent portion of amount (fraudsters often use .00 or .99) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "\n",
    "    # Log-amount (normalises the heavy right tail)\n",
    "    X[\"log_TransactionAmt\"] = np.log1p(X[\"TransactionAmt\"]).astype(np.float32)\n",
    "\n",
    "    # Cent portion of amount\n",
    "    X[\"amt_cents\"] = (X[\"TransactionAmt\"] % 1).round(2).astype(np.float32)\n",
    "\n",
    "    # Round-amount flag (fraudsters often use $1.00, $10.00, etc.)\n",
    "    X[\"is_round_amount\"] = (X[\"amt_cents\"] == 0).astype(np.int8)\n",
    "\n",
    "    # Address mismatch between addr1 and addr2 (if both present and valid)\n",
    "    if \"addr1\" in X.columns and \"addr2\" in X.columns:\n",
    "        valid = (X[\"addr1\"] != -999) & (X[\"addr2\"] != -999)\n",
    "        X[\"addr_mismatch\"] = ((X[\"addr1\"] != X[\"addr2\"]) & valid).astype(np.int8)\n",
    "\n",
    "    # D1 / D2 ratio (relative recency of last transaction)\n",
    "    if \"D1\" in X.columns and \"D2\" in X.columns:\n",
    "        d2_safe = X[\"D2\"].replace(-999, np.nan)\n",
    "        d1_safe = X[\"D1\"].replace(-999, np.nan)\n",
    "        X[\"D1_D2_ratio\"] = (d1_safe / (d2_safe + 1)).astype(np.float32)\n",
    "\n",
    "    # C1 × C2 interaction (two count features that together may signal fraud)\n",
    "    if \"C1\" in X.columns and \"C2\" in X.columns:\n",
    "        X[\"C1_x_C2\"] = (X[\"C1\"] * X[\"C2\"]).astype(np.float32)\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train = add_interaction_features(X_train)\n",
    "X_test  = add_interaction_features(X_test)\n",
    "\n",
    "new_cols = [\"log_TransactionAmt\", \"amt_cents\", \"is_round_amount\",\n",
    "            \"addr_mismatch\", \"D1_D2_ratio\", \"C1_x_C2\"]\n",
    "new_cols = [c for c in new_cols if c in X_train.columns]\n",
    "print(f\"Added {len(new_cols)} interaction features.\")\n",
    "display(X_train[new_cols].describe().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f11ee",
   "metadata": {},
   "source": [
    "## 7. Entity-Level Aggregation Features\n",
    "\n",
    "Fraud patterns are often *entity-level*: a stolen card is used many times\n",
    "in a short window, or a particular email provider is associated with\n",
    "unusually high transaction amounts.\n",
    "\n",
    "Group statistics capture this behaviour:\n",
    "\n",
    "```\n",
    "card1_TransactionAmt_mean  = average amount for this card number\n",
    "card1_TransactionAmt_std   = how volatile is this card's spending?\n",
    "card1_TransactionAmt_count = how many transactions has this card made?\n",
    "```\n",
    "\n",
    "A single transaction that is 5× the card's mean amount is a strong fraud signal\n",
    "— but the model can only discover this if the group mean is available as a feature.\n",
    "\n",
    "> **Key best practice:** fit the aggregator on train, transform both train and\n",
    "> test using stored means. Never let test-set rows contribute to their own group\n",
    "> statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8ed4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    # Card-level behaviour\n",
    "    {\"group_cols\": [\"card1\"],\n",
    "     \"agg_cols\": [\"TransactionAmt\"],\n",
    "     \"agg_funcs\": [\"mean\", \"std\", \"max\", \"count\"]},\n",
    "\n",
    "    # Card + product interaction\n",
    "    {\"group_cols\": [\"card1\", \"ProductCD\"],\n",
    "     \"agg_cols\": [\"TransactionAmt\"],\n",
    "     \"agg_funcs\": [\"mean\", \"count\"]},\n",
    "\n",
    "    # Email domain risk profiles\n",
    "    {\"group_cols\": [\"P_emaildomain\"],\n",
    "     \"agg_cols\": [\"TransactionAmt\"],\n",
    "     \"agg_funcs\": [\"mean\", \"std\", \"max\"]},\n",
    "\n",
    "    # Address-level patterns\n",
    "    {\"group_cols\": [\"addr1\"],\n",
    "     \"agg_cols\": [\"TransactionAmt\"],\n",
    "     \"agg_funcs\": [\"mean\", \"count\"]},\n",
    "\n",
    "    # D-feature (time between transactions) per card\n",
    "    {\"group_cols\": [\"card1\"],\n",
    "     \"agg_cols\": [\"D1\", \"D2\"],\n",
    "     \"agg_funcs\": [\"mean\", \"std\"]},\n",
    "]\n",
    "\n",
    "agg = GroupAggregator(groups=groups)\n",
    "X_train = agg.fit_transform(X_train)\n",
    "X_test  = agg.transform(X_test)\n",
    "\n",
    "agg_cols = [c for c in X_train.columns if any(\n",
    "    c.startswith(g[\"group_cols\"][0]) and c not in [g[\"group_cols\"][0]]\n",
    "    for g in groups\n",
    ")]\n",
    "print(f\"Aggregation features added. New shape: {X_train.shape}\")\n",
    "\n",
    "# Show a few\n",
    "agg_feature_names = agg.generated_feature_names\n",
    "agg_feature_names = [c for c in agg_feature_names if c in X_train.columns]\n",
    "display(X_train[agg_feature_names[:8]].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise: amount vs. card-level mean (fraud vs. legit)\n",
    "if \"card1_TransactionAmt_mean\" in X_train.columns:\n",
    "    plot_df = X_train[[\"TransactionAmt\", \"card1_TransactionAmt_mean\"]].copy()\n",
    "    plot_df[\"ratio\"] = plot_df[\"TransactionAmt\"] / (plot_df[\"card1_TransactionAmt_mean\"] + 1)\n",
    "    plot_df[\"isFraud\"] = y_train.values\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    for label, color, ls in [(0, \"steelblue\", \"-\"), (1, \"tomato\", \"--\")]:\n",
    "        data = plot_df.loc[plot_df[\"isFraud\"] == label, \"ratio\"].clip(0, 10)\n",
    "        sns.kdeplot(data, ax=ax, color=color, ls=ls,\n",
    "                    label=\"Legit\" if label == 0 else \"Fraud\", fill=True, alpha=0.3)\n",
    "    ax.set_xlabel(\"TransactionAmt / card-mean amount\")\n",
    "    ax.set_title(\"Distribution of amount-to-card-mean ratio\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c7e24",
   "metadata": {},
   "source": [
    "## 8. Temporal Feature Engineering\n",
    "\n",
    "`TransactionDT` is a *seconds offset* from a fixed reference date\n",
    "(2017-11-30 in the competition data). By mapping it to a calendar,\n",
    "we extract actionable signals:\n",
    "\n",
    "- **Hour of day** — fraud spikes at night (00:00–05:59)\n",
    "- **Day of week** — weekend spending patterns differ\n",
    "- **Is-weekend, Is-night** — binary fraud indicators\n",
    "\n",
    "### Cyclical encoding\n",
    "\n",
    "Naive linear encoding of `hour` treats 23 and 0 as far apart (distance 23),\n",
    "but they are only 1 hour apart. Cyclical sin/cos encoding maps the feature\n",
    "onto a unit circle so that 23:00 and 00:00 are close:\n",
    "\n",
    "```python\n",
    "sin_hour = sin(2π × hour / 24)\n",
    "cos_hour = cos(2π × hour / 24)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018399f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = extract_temporal_features(X_train, col=\"TransactionDT\", cyclical=True)\n",
    "X_test  = extract_temporal_features(X_test,  col=\"TransactionDT\", cyclical=True)\n",
    "\n",
    "temporal_cols = [\"hour\", \"day_of_week\", \"is_weekend\", \"is_night\",\n",
    "                 \"sin_hour\", \"cos_hour\", \"tx_age_days\"]\n",
    "temporal_cols = [c for c in temporal_cols if c in X_train.columns]\n",
    "print(f\"Temporal features extracted: {temporal_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a97f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud rate by hour of day\n",
    "plot_df = pd.DataFrame({\"hour\": X_train[\"hour\"], \"isFraud\": y_train.values})\n",
    "fraud_by_hour = plot_df.groupby(\"hour\")[\"isFraud\"].mean() * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4))\n",
    "fraud_by_hour.plot(kind=\"bar\", ax=ax, color=\"tomato\", edgecolor=\"white\", width=0.8)\n",
    "ax.set_xlabel(\"Hour of day\")\n",
    "ax.set_ylabel(\"Fraud rate (%)\")\n",
    "ax.set_title(\"Fraud rate by hour of day\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e736c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email-domain derived features\n",
    "X_train = extract_email_features(X_train)\n",
    "X_test  = extract_email_features(X_test)\n",
    "\n",
    "email_feats = [c for c in X_train.columns\n",
    "               if \"email\" in c and c not in [\"P_emaildomain\", \"R_emaildomain\"]]\n",
    "print(\"Email-derived features:\", email_feats)\n",
    "display(X_train[email_feats].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7595c4",
   "metadata": {},
   "source": [
    "## 9. PCA Compression of V-Feature Blocks\n",
    "\n",
    "The 339 V-features (V1–V339) were engineered by Vesta Corporation and\n",
    "grouped into correlated blocks with shared missingness patterns.\n",
    "Including all 339 raw columns has two problems:\n",
    "\n",
    "1. **High dimensionality** — many columns are nearly collinear.\n",
    "2. **Missing-value blocks** — an entire block may be -999 for a given row.\n",
    "\n",
    "**Strategy:** apply PCA *per block*, retaining 95 % of explained variance.\n",
    "This typically compresses 50–100 columns to 5–15 components while preserving\n",
    "the signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee15434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define V-feature blocks (mirrors the missingness structure)\n",
    "V_BLOCKS = {\n",
    "    \"v_blk1\":  [f\"V{i}\" for i in range(1,   12)],\n",
    "    \"v_blk2\":  [f\"V{i}\" for i in range(12,  35)],\n",
    "    \"v_blk3\":  [f\"V{i}\" for i in range(35,  53)],\n",
    "    \"v_blk4\":  [f\"V{i}\" for i in range(53,  75)],\n",
    "    \"v_blk5\":  [f\"V{i}\" for i in range(75,  95)],\n",
    "    \"v_blk6\":  [f\"V{i}\" for i in range(95,  138)],\n",
    "    \"v_blk7\":  [f\"V{i}\" for i in range(138, 167)],\n",
    "    \"v_blk8\":  [f\"V{i}\" for i in range(167, 217)],\n",
    "    \"v_blk9\":  [f\"V{i}\" for i in range(217, 279)],\n",
    "    \"v_blk10\": [f\"V{i}\" for i in range(279, 340)],\n",
    "}\n",
    "\n",
    "pca_objects = {}\n",
    "for block_name, v_cols in V_BLOCKS.items():\n",
    "    present = [c for c in v_cols if c in X_train.columns]\n",
    "    if len(present) < 2:\n",
    "        continue\n",
    "    X_train, pca_obj = pca_compress(\n",
    "        X_train, cols=present, n_components=0.95,\n",
    "        prefix=block_name, fit_df=X_train,\n",
    "    )\n",
    "    X_test, _ = pca_compress(\n",
    "        X_test, cols=present, n_components=pca_obj.n_components_,\n",
    "        prefix=block_name, fit_df=None,\n",
    "    )\n",
    "    # For test we need to apply the already-fitted PCA\n",
    "    # (pca_compress with int n_components and fit_df=None won't refit)\n",
    "    pca_objects[block_name] = pca_obj\n",
    "\n",
    "print(f\"\\nShape after PCA compression: {X_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise explained variance for one block\n",
    "if \"v_blk1\" in pca_objects:\n",
    "    plot_pca_explained_variance(pca_objects[\"v_blk1\"],\n",
    "                                title=\"PCA — V-block 1 (V1–V11)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0a16d",
   "metadata": {},
   "source": [
    "## 10. Feature Selection\n",
    "\n",
    "With hundreds of engineered features, removing redundant ones:\n",
    "- Speeds up training\n",
    "- Reduces overfitting (especially for linear models)\n",
    "- Improves interpretability\n",
    "\n",
    "We apply three complementary filters in sequence:\n",
    "\n",
    "1. **Variance threshold** — drop near-constant features (zero variance).\n",
    "2. **Correlation pruning** — drop one of each highly-correlated pair (|ρ| > 0.95).\n",
    "3. **Permutation importance** — keep the top-N most predictive features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep numeric columns for the selection steps\n",
    "X_train_num = X_train.select_dtypes(include=[np.number])\n",
    "X_test_num  = X_test[X_train_num.columns]\n",
    "\n",
    "print(f\"Starting features: {X_train_num.shape[1]}\")\n",
    "\n",
    "# Step 1: Variance threshold\n",
    "X_train_num = variance_threshold_select(X_train_num, threshold=0.0,\n",
    "                                        exclude=[\"isFraud\"])\n",
    "X_test_num  = X_test_num[X_train_num.columns]\n",
    "print(f\"After variance filter: {X_train_num.shape[1]}\")\n",
    "\n",
    "# Step 2: Correlation pruning\n",
    "print(\"\\nCorrelation pruning (|ρ| > 0.95):\")\n",
    "X_train_num = drop_high_correlation(X_train_num, threshold=0.95, verbose=True)\n",
    "X_test_num  = X_test_num[X_train_num.columns]\n",
    "print(f\"After correlation filter: {X_train_num.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2768b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Permutation importance with a fast RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_selector = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=8, n_jobs=-1, random_state=42\n",
    ")\n",
    "rf_selector.fit(X_train_num, y_train)\n",
    "\n",
    "X_train_selected, imp_series = importance_select(\n",
    "    X_train_num, y_train, model=rf_selector, top_n=60,\n",
    ")\n",
    "X_test_selected = X_test_num[X_train_selected.columns]\n",
    "\n",
    "print(f\"After importance filter (top 60): {X_train_selected.shape[1]}\")\n",
    "\n",
    "plot_feature_importance(\n",
    "    imp_series, top_n=30,\n",
    "    title=\"Permutation importance — top 30 features\",\n",
    "    figsize=(10, 9),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e61a2",
   "metadata": {},
   "source": [
    "## 11. Baseline vs. Engineered Features — Model Lift\n",
    "\n",
    "We compare two XGBoost classifiers:\n",
    "\n",
    "- **Baseline** — raw numeric columns only, minimal preprocessing.\n",
    "- **Engineered** — full feature set built in §§3–10.\n",
    "\n",
    "This quantifies the value-add of feature engineering beyond raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc91b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Baseline: raw transaction columns only ────────────────────────────────────\n",
    "BASELINE_COLS = (\n",
    "    [\"TransactionAmt\", \"TransactionDT\"]\n",
    "    + [f\"card{i}\" for i in range(1, 7)]\n",
    "    + [f\"C{i}\" for i in range(1, 15)]\n",
    "    + [f\"D{i}\" for i in range(1, 16)]\n",
    ")\n",
    "\n",
    "# Load fresh (un-engineered) splits\n",
    "X_raw = df.drop(columns=[\"isFraud\", \"TransactionID\"])\n",
    "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(\n",
    "    X_raw, df[\"isFraud\"], test_size=0.20, random_state=42, stratify=df[\"isFraud\"]\n",
    ")\n",
    "\n",
    "base_cols_present = [c for c in BASELINE_COLS\n",
    "                     if c in X_raw_train.select_dtypes(include=np.number).columns]\n",
    "X_base_train = X_raw_train[base_cols_present].fillna(-999)\n",
    "X_base_test  = X_raw_test[base_cols_present].fillna(-999)\n",
    "\n",
    "print(f\"Baseline feature count : {X_base_train.shape[1]}\")\n",
    "print(f\"Engineered feature count: {X_train_selected.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42039476",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = dict(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=5,\n",
    "    scale_pos_weight=(y_train == 0).sum() / (y_train == 1).sum(),\n",
    "    random_state=42,\n",
    "    eval_metric=\"auc\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Baseline model\n",
    "xgb_base = XGBClassifier(**xgb_params)\n",
    "xgb_base.fit(X_base_train, y_raw_train)\n",
    "\n",
    "# Engineered-features model\n",
    "xgb_eng = XGBClassifier(**xgb_params)\n",
    "xgb_eng.fit(X_train_selected, y_train)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = evaluate_model(\n",
    "    xgb_base, X_base_train, y_raw_train,\n",
    "    X_base_test,  y_raw_test,\n",
    "    name=\"XGBoost — Baseline features\",\n",
    ")\n",
    "\n",
    "results_eng = evaluate_model(\n",
    "    xgb_eng, X_train_selected, y_train,\n",
    "    X_test_selected, y_test,\n",
    "    name=\"XGBoost — Engineered features\",\n",
    ")\n",
    "\n",
    "lift = results_eng[\"test_auc\"] - results_base[\"test_auc\"]\n",
    "print(f\"\\n>>> AUC lift from feature engineering: {lift:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC comparison (handles different feature sets per model)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "pairs = [\n",
    "    (xgb_base, X_base_test,      y_raw_test, \"Baseline\",   \"steelblue\"),\n",
    "    (xgb_eng,  X_test_selected,  y_test,     \"Engineered\", \"tomato\"),\n",
    "]\n",
    "for model, X_t, y_t, label, color in pairs:\n",
    "    proba = model.predict_proba(X_t)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_t, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=color, lw=2.5,\n",
    "            label=f\"{label}  (AUC = {roc_auc:.4f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC — Baseline vs. Feature-Engineered Model\")\n",
    "ax.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af20370",
   "metadata": {},
   "source": [
    "## 12. Summary & Best-Practice Checklist\n",
    "\n",
    "The table below distils the key lessons from this notebook.\n",
    "\n",
    "| Practice | Implementation | Why it matters |\n",
    "|----------|----------------|----------------|\n",
    "| **Fit-on-train-only** | All encoders, scalers, and aggregators fitted exclusively on `X_train` | Prevents target leakage |\n",
    "| **Sentinel imputation** | `-999` fill for trees; median + `_was_nan` for linear models | Lets trees learn missingness as a signal |\n",
    "| **Smoothed target encoding** | `TargetEncoder(smoothing=20)` with LOO | Avoids overfitting on rare categories |\n",
    "| **Frequency encoding** | `FrequencyEncoder` | Captures category prevalence without leakage |\n",
    "| **Group aggregations** | `GroupAggregator` | Encodes entity-level behaviour |\n",
    "| **Cyclical temporals** | `sin/cos` pairs for hour, day-of-week, month | Distances are meaningful for periodic features |\n",
    "| **PCA per V-block** | `pca_compress` per block | Removes collinearity; respects missingness structure |\n",
    "| **Correlation pruning** | `drop_high_correlation(threshold=0.95)` | Removes redundancy without harming signal |\n",
    "| **Permutation importance** | `importance_select(top_n=60)` | Model-agnostic, avoids bias toward high-cardinality |\n",
    "| **`scale_pos_weight`** | Inverse class-frequency ratio | Handles 3.5 % fraud imbalance |\n",
    "| **Memory optimisation** | `reduce_mem_usage` | Essential for 590 k × 394 column data |\n",
    "\n",
    "### Next steps (not covered here)\n",
    "- **Time-aware cross-validation** — `TimeSeriesSplit` to simulate production conditions.\n",
    "- **Target-encoded features in CV** — use `cross_val_predict` with target encoder inside each fold.\n",
    "- **UMAP / t-SNE** — unsupervised exploration of the V-feature latent space.\n",
    "- **Neural entity embeddings** — learn card and email embeddings end-to-end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
